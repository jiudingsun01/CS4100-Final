{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAS VERSION - MIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, concatenate_datasets\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm, trange\n",
    "import nnsight\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pyvene.models.configuration_intervenable_model import  IntervenableConfig\n",
    "from pyvene.models.intervenable_base import IntervenableModel\n",
    "from pyvene.models.interventions import BoundlessRotatedSpaceIntervention\n",
    "from pyvene.models.basic_utils import set_seed, count_parameters\n",
    "\n",
    "\n",
    "from utils.prompt_utils import *\n",
    "from utils.intervention_utils import *\n",
    "from utils.model_utils import *\n",
    "from utils.eval_utils import *\n",
    "from utils.extract_utils import *\n",
    "from das_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names= [\n",
    "        \"antonym\",\n",
    "        \"capitalize\",\n",
    "        \"english-french\",\n",
    "        \"present-past\",\n",
    "        \"singular-plural\"\n",
    "    ]\n",
    "\n",
    "edit_layer = 0\n",
    "# model_name=\"/share/projects/engine/weights/llama-hf/llama-7b\"\n",
    "# model_name = \"luodian/llama-7b-hf\"\n",
    "model_name = \"../flan-llama-7b\"\n",
    "root_data_dir= \"../dataset_files\"\n",
    "save_path_root= \"../results/DAS/llama-7b\"+f\"/L{str(edit_layer)}\"\n",
    "intervention_path= None\n",
    "seed=42\n",
    "device= \"cuda\"\n",
    "test_split= 0.3\n",
    "n_shots= 10\n",
    "n_trials= 512\n",
    "prefixes= {\n",
    "    \"input\": \"Q:\",\n",
    "    \"output\": \"A:\",\n",
    "    \"instructions\": \"\"\n",
    "}\n",
    "separators= {\n",
    "    \"input\": \"\\n\",\n",
    "    \"output\": \"\\n\\n\",\n",
    "    \"instructions\": \"\"\n",
    "}\n",
    "training_method= \"both\"\n",
    "batch_size= 32\n",
    "gradient_accumulation_steps= 1\n",
    "epochs= 25\n",
    "warnup_ratio= 0.1\n",
    "rotate_lr= 0.001\n",
    "boundary_lr= 0.01\n",
    "temperature_start= 50.0\n",
    "temperature_end= 0.1\n",
    "evaluate_per_epoch= True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading:  ../flan-llama-7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee765597c2b433e98fbfbdab8f97e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the intervention model from /home/local_erictodd/Projects/subspace/models/L12/checkpoints/epoch_24...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The key is provided in the config. Assuming this is loaded from a pretrained module.\n",
      "WARNING:root:Loading trainable intervention from intkey_layer.12.repr.block_output.unit.pos.nunit.1#0.bin.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, model_config = load_gpt_model_and_tokenizer(model_name, device=device)\n",
    "    \n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "edit_layer = 12\n",
    "intervention_path = '/home/local_erictodd/Projects/subspace/models/L12/checkpoints/epoch_24'\n",
    "\n",
    "assert edit_layer < model_config[\"n_layers\"], f\"Edit layer {edit_layer} is out of range for model with {model_config['n_layers']} layers.\"\n",
    "\n",
    "if intervention_path is not None:\n",
    "    print(f\"Loading the intervention model from {intervention_path}...\")\n",
    "    intervenable_from = IntervenableModel.load(intervention_path, model=model)\n",
    "    \n",
    "    assert len(intervenable_from.interventions.keys()) == 1, \"No interventions found in the loaded model.\"\n",
    "    \n",
    "    intervention_key = list(intervenable_from.interventions.keys())[0]\n",
    "    layer_num = re.findall(r'(?<=layer.)\\d+', intervention_key)\n",
    "    assert layer_num is not None and len(layer_num) == 1, \"No layer number found in the loaded model.\"\n",
    "    layer_num = int(layer_num[0])\n",
    "    \n",
    "    if layer_num != edit_layer:\n",
    "        print(f\"WARNING: The loaded intervention model is for layer {layer_num} but you are running for layer {edit_layer}.\")\n",
    "        print(\"Loading the weight\")\n",
    "        intervenable_config = simple_boundless_das_position_config(type(model), \"block_output\", edit_layer)\n",
    "        intervenable = IntervenableModel(intervenable_config, model)\n",
    "        intervenable = load_intervention_weight(intervenable, intervenable_from)\n",
    "    else:\n",
    "        intervenable = intervenable_from\n",
    "else:\n",
    "    intervenable_config = simple_boundless_das_position_config(type(model), \"block_output\", edit_layer)\n",
    "    intervenable = IntervenableModel(intervenable_config, model)\n",
    "    \n",
    "intervenable.set_device(device)\n",
    "intervenable.disable_model_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundlessRotatedSpaceIntervention(\n",
       "  (rotate_layer): ParametrizedRotateLayer(\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): _Orthogonal()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model = nnsight.LanguageModel(model_name, device_map='cuda:0', dispatch=True)\n",
    "for p in nn_model.parameters():\n",
    "    p.requires_grad = False\n",
    "learned_proj = BoundlessRotatedSpaceIntervention(embed_dim=nn_model.config.hidden_size)\n",
    "learned_proj.load_state_dict(torch.load('../results/proj_l12_state_dict.pt'))\n",
    "learned_proj.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.load('match_das_results/inputs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Q: manipulate\\nA: manip</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p1 = 'Q: control\\nA:'\n",
    "# p2 = \n",
    "\n",
    "\n",
    "# nn_model.tokenizer.decode(inputs['base_input_ids'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model\n",
      "Loading:  luodian/llama-7b-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d6efab6eb34523898b2012e4abfc8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset\n",
      "Processing Dataloaders\n",
      "Evaluating the model 10-shots without intervention...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:15<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING: THIS NEEDS TO BE GOOD!] prealign task accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "\n",
    "\n",
    "# Load Model & Tokenizer\n",
    "print(\"Loading Model\")\n",
    "# model, tokenizer, model_config = load_gpt_model_and_tokenizer(model_name, device=device)\n",
    "\n",
    "# # tokenizer.pad_token = tokenizer.eos_token\n",
    "# # tokenizer.padding_side = \"right\"\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# assert edit_layer < model_config[\"n_layers\"], f\"Edit layer {edit_layer} is out of range for model with {model_config['n_layers']} layers.\"\n",
    "\n",
    "# if intervention_path is not None:\n",
    "#     print(f\"Loading the intervention model from {intervention_path}...\")\n",
    "#     intervenable = IntervenableModel.load(intervention_path, model=model)\n",
    "# else:\n",
    "#     intervenable_config = simple_boundless_das_position_config(type(model), \"block_output\", edit_layer)\n",
    "#     intervenable = IntervenableModel(intervenable_config, model)\n",
    "    \n",
    "# intervenable.set_device(device)\n",
    "# intervenable.disable_model_gradients()\n",
    "\n",
    "\n",
    "nn_model = nnsight.LanguageModel(model_name, device_map='cuda:0', dispatch=True)\n",
    "# nn_model = nnsight.LanguageModel(model, device_map='cuda:0', tokenizer=tokenizer)\n",
    "for p in nn_model.parameters():\n",
    "    p.requires_grad = False\n",
    "learnable_proj = BoundlessRotatedSpaceIntervention(embed_dim=model.config.hidden_size)\n",
    "learnable_proj.to('cuda')\n",
    "    \n",
    "def vanilla_collate_fn(batch):\n",
    "    input_ids, labels = tuple([data_pair[key] for data_pair in batch] for key in (\"input_ids\", \"labels\"))\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "    \n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
    "    )\n",
    "    \n",
    "def intervention_collate_fn(batch):\n",
    "    base_input_ids, base_labels, source_input_ids, source_labels, source_predictive_token_idxs, predictive_token_idxs = tuple(\n",
    "        [data_pair[key] for data_pair in batch] for key in \n",
    "        ('base_input_ids', 'base_labels', 'source_input_ids', 'source_labels', 'source_predictive_token_idxs', 'predictive_token_idxs')\n",
    "    )\n",
    "    \n",
    "    base_input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        base_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    source_input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        source_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    base_labels = torch.nn.utils.rnn.pad_sequence(base_labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "    source_labels = torch.nn.utils.rnn.pad_sequence(source_labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "    \n",
    "    source_predictive_token_idxs = torch.LongTensor(source_predictive_token_idxs)\n",
    "    predictive_token_idxs = torch.LongTensor(predictive_token_idxs)\n",
    "    \n",
    "    return dict(\n",
    "        base_input_ids=base_input_ids,\n",
    "        base_labels=base_labels,\n",
    "        base_attention_mask=base_input_ids.ne(tokenizer.pad_token_id),\n",
    "        source_input_ids=source_input_ids,\n",
    "        source_labels=source_labels,\n",
    "        source_attention_mask=source_input_ids.ne(tokenizer.pad_token_id),\n",
    "        predictive_token_idxs=predictive_token_idxs,\n",
    "        source_predictive_token_idxs=source_predictive_token_idxs\n",
    "    )\n",
    "\n",
    "def calculate_loss(logits, labels):\n",
    "    shift_logits = logits[..., :, :].contiguous()\n",
    "    shift_labels = labels[..., :].contiguous()\n",
    "    # Flatten the tokens\n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    shift_logits = shift_logits.view(-1, intervenable.model_config.vocab_size)\n",
    "    shift_labels = shift_labels.view(-1)\n",
    "    # Enable model parallelism\n",
    "    shift_labels = shift_labels.to(shift_logits.device)\n",
    "    loss = loss_fct(shift_logits, shift_labels)\n",
    "    \n",
    "    for k, v in intervenable.interventions.items():\n",
    "        boundary_loss = 1. * v[0].intervention_boundaries.sum()\n",
    "    loss += boundary_loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading Dataset\")\n",
    "set_seed(seed)\n",
    "datasets = [load_dataset(dataset_name, root_data_dir=root_data_dir, test_size=test_split, seed=seed) for dataset_name in dataset_names]\n",
    "\n",
    "if not os.path.exists(save_path_root):\n",
    "    os.makedirs(save_path_root)\n",
    "    \n",
    "print(\"Processing Dataloaders\")\n",
    "\n",
    "eval_no_intervention_dataloader = process_mixed_dataloader(datasets, model_config, tokenizer, batch_size, n_shots, \"valid\", prefixes, separators, intervention_collate_fn, ablation_method=\"zero_shot\", draw_source_from_split=True)\n",
    "if training_method == \"both\":\n",
    "    \n",
    "    all_datasets = []\n",
    "    for method in [\"zero_shot\", \"noninformative\"]:\n",
    "        for dataset in datasets:\n",
    "            all_datasets.append(process_dataset(dataset, model_config, tokenizer, n_shots, \"train\", prefixes, separators, n_trials=n_trials, ablation_method=method, draw_source_from_split=False))\n",
    "    train_dataset = concatenate_datasets(all_datasets)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=intervention_collate_fn)\n",
    "else:\n",
    "    train_dataloader = process_mixed_dataloader(datasets, model_config, tokenizer, batch_size, n_shots, \"train\", prefixes, separators, intervention_collate_fn, n_trials=n_trials, ablation_method=training_method, draw_source_from_split=False)\n",
    "    \n",
    "fs_eval_dataloader = process_mixed_dataloader(datasets, model_config, tokenizer, batch_size, n_shots, \"valid\", prefixes, separators, intervention_collate_fn, ablation_method=\"noninformative\", draw_source_from_split=True)\n",
    "zs_eval_dataloader = process_mixed_dataloader(datasets, model_config, tokenizer, batch_size, n_shots, \"valid\", prefixes, separators, intervention_collate_fn, ablation_method=\"zero_shot\", draw_source_from_split=True)\n",
    "\n",
    "print(f\"Evaluating the model {n_shots}-shots without intervention...\")\n",
    "eval_accuracy = evaluate(intervenable, eval_no_intervention_dataloader, device=model.device, intervene=False, corrupt=False)\n",
    "print(f\"[WARNING: THIS NEEDS TO BE GOOD!] prealign task accuracy: {eval_accuracy}\")\n",
    "results['prealign_val_task_accuracy'] = eval_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama trainable parameters:  0\n",
      "intervention trainable parameters:  16777218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "t_total = int(len(train_dataloader) * epochs)\n",
    "warm_up_steps = 0.1 * t_total\n",
    "optimizer_params = []\n",
    "\n",
    "for k, v in intervenable.interventions.items():\n",
    "    optimizer_params += [{'params': v[0].rotate_layer.parameters()}]\n",
    "    optimizer_params += [{'params': v[0].intervention_boundaries, 'lr': boundary_lr}]\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    optimizer_params,\n",
    "    lr=rotate_lr,\n",
    ")\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warm_up_steps,\n",
    "    num_training_steps=t_total\n",
    ")\n",
    "\n",
    "target_total_step = len(train_dataloader) * epochs\n",
    "\n",
    "temperature_schedule = torch.linspace(\n",
    "    temperature_start, temperature_end, target_total_step\n",
    ").to(torch.bfloat16).to(device)\n",
    "\n",
    "total_step = 0\n",
    "intervenable.set_temperature(temperature_schedule[total_step])\n",
    "\n",
    "intervenable.model.train() # train enables drop-off but no grads\n",
    "print(\"llama trainable parameters: \", count_parameters(intervenable.model))\n",
    "print(\"intervention trainable parameters: \", intervenable.count_parameters())\n",
    "\n",
    "train_iterator = trange(\n",
    "    0, int(epochs), desc=\"Epoch\"\n",
    ")\n",
    "\n",
    "training_log_dicts = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0:   0%|          | 0/160 [00:04<?, ?it/s, loss=3.1, acc=0.12, sparsity=0.492]\n",
      "Epoch:   0%|          | 0/25 [00:04<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "if intervention_path is None:\n",
    "    \n",
    "    os.makedirs(os.path.join(save_path_root, \"checkpoints\"), exist_ok=True)\n",
    "    \n",
    "    training_log_dicts = []\n",
    "            \n",
    "    for epoch in train_iterator:\n",
    "        \n",
    "        log_dicts = []\n",
    "        ckpt_path = os.path.join(save_path_root, \"checkpoints\", f\"epoch_{epoch}\")\n",
    "        \n",
    "        epoch_iterator = tqdm(\n",
    "            train_dataloader, desc=f\"Epoch: {epoch}\", position=0, leave=True\n",
    "        )\n",
    "        \n",
    "        for step, inputs in enumerate(epoch_iterator):\n",
    "            for k, v in inputs.items():\n",
    "                if v is not None and isinstance(v, torch.Tensor):\n",
    "                    inputs[k] = v.to(device)\n",
    "            b_s = inputs[\"base_input_ids\"].shape[0]\n",
    "\n",
    "            source2base = ([[[idx] for idx in inputs[\"source_predictive_token_idxs\"].tolist()]], [[[idx] for idx in inputs[\"predictive_token_idxs\"].tolist()]])\n",
    "            \n",
    "            _, counterfactual_outputs = intervenable(\n",
    "                {\"input_ids\": inputs[\"base_input_ids\"], \"attention_mask\": inputs[\"base_attention_mask\"]},\n",
    "                [{\"input_ids\": inputs[\"source_input_ids\"], \"attention_mask\": inputs[\"source_attention_mask\"]}],\n",
    "                {\"sources->base\": source2base}\n",
    "            )\n",
    "            \n",
    "            eval_metrics = compute_metrics(\n",
    "                [counterfactual_outputs.logits], [inputs['base_labels']]\n",
    "            )\n",
    "            \n",
    "            loss = calculate_loss(\n",
    "                counterfactual_outputs.logits, inputs[\"base_labels\"]\n",
    "            )\n",
    "            loss_str = round(loss.item(), 2)\n",
    "            \n",
    "            log_dict = {'loss': loss_str, 'acc': eval_metrics[\"accuracy\"], 'sparsity': compute_rotation_mask_sparsity(intervenable)}\n",
    "            epoch_iterator.set_postfix(log_dict)\n",
    "            \n",
    "            log_dicts.append(log_dict)\n",
    "            \n",
    "            if gradient_accumulation_steps > 1:\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "                \n",
    "            loss.backward()\n",
    "            # if total_step % gradient_accumulation_steps == 0:\n",
    "            #     if not (gradient_accumulation_steps > 1 and total_step == 0):\n",
    "            #         optimizer.step()\n",
    "            #         scheduler.step()\n",
    "            #         intervenable.set_zero_grad()\n",
    "            #         intervenable.set_temperature(temperature_schedule[total_step])\n",
    "                    \n",
    "            # total_step += 1\n",
    "            break\n",
    "        break\n",
    "\n",
    "        \n",
    "#         ave_loss = round(sum([log_dict['loss'] for log_dict in log_dicts])/len(log_dicts), 4)\n",
    "#         ave_acc = round(sum([log_dict['acc'] for log_dict in log_dicts])/len(log_dicts), 4)\n",
    "#         ave_sparsity = round(sum([log_dict['sparsity'] for log_dict in log_dicts])/len(log_dicts), 4) \n",
    "        \n",
    "#         epoch_training_log = {'loss': ave_loss, 'acc': ave_acc, 'sparsity': ave_sparsity}\n",
    "#         print(\"Epoch \" + str(epoch) + \" finished! Training loss: \" + str(ave_loss) + \", training acc: \" + str(ave_acc) + \", sparsity: \" + str(ave_sparsity))\n",
    "        \n",
    "#         if evaluate_per_epoch:\n",
    "            \n",
    "#             fs_shuffled_acc = evaluate(intervenable, fs_eval_dataloader, device=model.device, intervene=True)\n",
    "#             epoch_training_log['fs_shuffled_with_intervention_accuracy'] = fs_shuffled_acc\n",
    "            \n",
    "#             zs_intervention_acc = evaluate(intervenable, zs_eval_dataloader, device=model.device, intervene=True)\n",
    "#             epoch_training_log['zs_with_intervention_accuracy'] = zs_intervention_acc\n",
    "            \n",
    "#             print(\"Few-shot shuffled with intervention accuracy: \" + str(epoch_training_log['fs_shuffled_with_intervention_accuracy']))\n",
    "#             print(\"Zero-shot with intervention accuracy: \" + str(epoch_training_log['zs_with_intervention_accuracy']))\n",
    "        \n",
    "#         intervenable.save(ckpt_path)\n",
    "#         training_log_dicts.append(epoch_training_log)\n",
    "    \n",
    "# print(\"Evaluation the model with intervention...\")\n",
    "\n",
    "# fs_shuffled_acc = evaluate(intervenable, fs_eval_dataloader, device=model.device, intervene=True)\n",
    "# results['fs_shuffled_with_intervention_accuracy'] = fs_shuffled_acc\n",
    "\n",
    "# fs_shuffled_no_intervention_acc = evaluate(intervenable, fs_eval_dataloader, device=model.device, intervene=False, corrupt=True)\n",
    "# results['fs_shuffled_no_intervention_accuracy'] = fs_shuffled_no_intervention_acc\n",
    "\n",
    "# zs_intervention_acc = evaluate(intervenable, zs_eval_dataloader, device=model.device, intervene=True)\n",
    "# results['zs_with_intervention_accuracy'] = zs_intervention_acc\n",
    "    \n",
    "# zs_no_intervention_acc = evaluate(intervenable, zs_eval_dataloader, device=model.device, intervene=False, corrupt=True)\n",
    "# results['zs_no_intervention_accuracy'] = zs_no_intervention_acc\n",
    "\n",
    "# print(\"Few-shot shuffled with intervention accuracy: \" + str(results['fs_shuffled_with_intervention_accuracy']))\n",
    "# print(\"Few-shot shuffled no intervention accuracy: \" + str(results['fs_shuffled_no_intervention_accuracy']))\n",
    "# print(\"Zero-shot with intervention accuracy: \" + str(results['zs_with_intervention_accuracy']))\n",
    "# print(\"Zero-shot no intervention accuracy: \" + str(results['zs_no_intervention_accuracy']))\n",
    "\n",
    "# print(\"Saving results...\")\n",
    "\n",
    "# with open(f\"{save_path_root}/args.json\", \"w\") as f:\n",
    "#     json.dump(vars(args), f, indent=4)\n",
    "#     f.close()\n",
    "    \n",
    "# with open(f\"{save_path_root}/results.json\", \"w\") as f:\n",
    "#     json.dump(results, f, indent=4)\n",
    "    \n",
    "# if training_log_dicts is not None:\n",
    "#     with open(f\"{save_path_root}/training_log.json\", \"w\") as f:\n",
    "#         json.dump(training_log_dicts, f, indent=4)\n",
    "#         f.close()\n",
    "        \n",
    "# intervenable.save(f\"{save_path_root}/intervention_model\")\n",
    "# print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNSIGHT MODEL TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, concatenate_datasets\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm, trange\n",
    "import nnsight\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from pyvene.models.configuration_intervenable_model import  IntervenableConfig, RepresentationConfig\n",
    "# from pyvene.models.intervenable_base import IntervenableModel\n",
    "from pyvene.models.interventions import BoundlessRotatedSpaceIntervention\n",
    "# from pyvene.models.basic_utils import set_seed, count_parameters\n",
    "\n",
    "\n",
    "from utils.prompt_utils import *\n",
    "from utils.intervention_utils import *\n",
    "from utils.model_utils import *\n",
    "from utils.eval_utils import *\n",
    "from utils.extract_utils import *\n",
    "from das_utils import *\n",
    "\n",
    "dataset_names= [\n",
    "        \"antonym\",\n",
    "        \"capitalize\",\n",
    "        \"english-french\",\n",
    "        \"present-past\",\n",
    "        \"singular-plural\"\n",
    "    ]\n",
    "\n",
    "edit_layer = 0\n",
    "# model_name=\"/share/projects/engine/weights/llama-hf/llama-7b\"\n",
    "model_name = \"luodian/llama-7b-hf\"\n",
    "root_data_dir= \"../dataset_files\"\n",
    "save_path_root= \"../results/DAS/llama-7b\"+f\"/L{str(edit_layer)}\"\n",
    "intervention_path= None\n",
    "seed=42\n",
    "device= \"cuda\"\n",
    "test_split= 0.3\n",
    "n_shots= 10\n",
    "n_trials= 512\n",
    "prefixes= {\n",
    "    \"input\": \"Q:\",\n",
    "    \"output\": \"A:\",\n",
    "    \"instructions\": \"\"\n",
    "}\n",
    "separators= {\n",
    "    \"input\": \"\\n\",\n",
    "    \"output\": \"\\n\\n\",\n",
    "    \"instructions\": \"\"\n",
    "}\n",
    "training_method= \"both\"\n",
    "batch_size= 32\n",
    "gradient_accumulation_steps= 1\n",
    "epochs= 25\n",
    "warnup_ratio= 0.1\n",
    "rotate_lr= 0.001\n",
    "boundary_lr= 0.01\n",
    "temperature_start= 50.0\n",
    "temperature_end= 0.1\n",
    "evaluate_per_epoch= True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotateLayer(torch.nn.Module):\n",
    "    \"\"\"A linear transformation with orthogonal initialization.\"\"\"\n",
    "\n",
    "    def __init__(self, n, init_orth=True):\n",
    "        super().__init__()\n",
    "        weight = torch.empty(n, n)\n",
    "        if init_orth:\n",
    "            torch.nn.init.orthogonal_(weight)\n",
    "        self.weight = torch.nn.Parameter(weight, requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.matmul(x.to(self.weight.dtype), self.weight)\n",
    "    \n",
    "    \n",
    "class PyveneIntervention(torch.nn.Module):\n",
    "    \"\"\"Intervention in the rotated space with boundary mask.\"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, steps, init_value=100.0, random_mask=False):\n",
    "        super(PyveneIntervention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        rotate_layer = RotateLayer(self.embed_dim)\n",
    "        self.rotate_layer = torch.nn.utils.parametrizations.orthogonal(rotate_layer)\n",
    "        \n",
    "        if not random_mask:\n",
    "            self.mask = torch.nn.Parameter(torch.tensor([init_value] * self.embed_dim), requires_grad=True) # [100.0]\n",
    "        else:\n",
    "            self.mask = torch.nn.Parameter((-100 - 100) * torch.rand(self.embed_dim) + 100) # uniform on [-100, 100]\n",
    "        \n",
    "        self.temp_range = torch.linspace(torch.tensor(50.0), torch.tensor(0.10), steps+1).to('cuda')\n",
    "        self.curr_step = 0\n",
    "        self.temperature = torch.nn.Parameter(torch.tensor(50.0)).to('cuda')\n",
    "\n",
    "    def get_temperature(self):\n",
    "        return self.temperature\n",
    "\n",
    "    def update_temp(self):\n",
    "        self.curr_step += 1\n",
    "        self.temperature.data = self.temp_range[self.curr_step]\n",
    "    \n",
    "    def effective_subspace_dim(self):\n",
    "        return sum(torch.sigmoid(self.mask / self.temperature)).item()\n",
    "\n",
    "    def forward(self, source, base):\n",
    "\n",
    "        rotated_base = self.rotate_layer(base)\n",
    "        rotated_source = self.rotate_layer(source)\n",
    "        # get boundary mask between 0 and 1 from sigmoid\n",
    "        boundary_mask = torch.sigmoid(self.mask / self.temperature)\n",
    "\n",
    "        boundary_mask = (\n",
    "            # torch.ones_like(base, device=base.device)[:,0,0].unsqueeze(dim=-1) * boundary_mask\n",
    "            # torch.ones(base.shape[0], device=base.device).unsqueeze(dim=-1) * boundary_mask\n",
    "            torch.ones_like(base)[:,0].unsqueeze(dim=-1).to(base.device) * boundary_mask\n",
    "        )\n",
    "        boundary_mask = boundary_mask.to(rotated_base.dtype)\n",
    "        # interchange\n",
    "        rotated_output = (\n",
    "            1.0 - boundary_mask\n",
    "        ) * rotated_base + boundary_mask * rotated_source\n",
    "        # inverse output\n",
    "        output = torch.matmul(rotated_output, self.rotate_layer.weight.T)\n",
    "        return output.to(base.dtype).squeeze()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"PyveneIntervention()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.load('match_das_results/inputs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa1b1662df94c31b3d43461ff8d4e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PyveneIntervention(\n",
       "  (rotate_layer): ParametrizedRotateLayer(\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): _Orthogonal()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model = nnsight.LanguageModel(model_name, device_map='cuda:0', dispatch=True)\n",
    "for p in nn_model.parameters():\n",
    "    p.requires_grad = False\n",
    "learned_proj = PyveneIntervention(embed_dim=nn_model.config.hidden_size, steps=10000)\n",
    "# learned_proj = BoundlessRotatedSpaceIntervention(embed_dim=nn_model.config.hidden_size)\n",
    "# learned_proj.load_state_dict(torch.load('../results/proj_l12_state_dict.pt'))\n",
    "learned_proj.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "def intervene_llama_DAS_batch(batch, layer, model, ii_proj):\n",
    "\n",
    "\n",
    "    # batch_size = len(batch['base_input_ids'])\n",
    "    batch_size = 8\n",
    "    all_inds = torch.arange(batch_size)\n",
    "        \n",
    "    base_prompt, source_prompt = batch['base_input_ids'][:batch_size], batch['source_input_ids'][:batch_size]\n",
    "    base_intervention_token_idx, source_intervention_token_idx = batch['predictive_token_idxs'][:batch_size], batch['source_predictive_token_idxs'][:batch_size]\n",
    "\n",
    "    with model.trace(validate=False) as tracer:\n",
    "\n",
    "        with tracer.invoke(base_prompt, scan=False):\n",
    "            base = model.model.layers[layer].output[0]\n",
    "        \n",
    "        with tracer.invoke(source_prompt, scan=False):\n",
    "            source = model.model.layers[layer].output[0]\n",
    "\n",
    "        # intervention\n",
    "        with tracer.invoke(base_prompt, scan=False):\n",
    "            B = base[all_inds,base_intervention_token_idx,:]\n",
    "            S = source[all_inds,source_intervention_token_idx,:]\n",
    "\n",
    "            mixed_out = ii_proj(S, B)\n",
    "            model.model.layers[layer].output[0][all_inds,base_intervention_token_idx,:] = mixed_out\n",
    "            del base, source, B,S\n",
    "        save_out = model.output.save()\n",
    "    \n",
    "    \n",
    "    # print(b_shape)\n",
    "    output_logits = save_out.value.logits[1::3]\n",
    "    del save_out\n",
    "    return output_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attn_in = []\n",
    "# attn_out = []\n",
    "# with nn_model.trace() as tracer:\n",
    "#     with tracer.invoke('hello world base'):\n",
    "#         for i in range(nn_model.config.num_hidden_layers):\n",
    "#             source_attn_in = nn_model.model.layers[i].self_attn.o_proj.input.save()\n",
    "            \n",
    "#     with tracer.invoke('hello world source'):\n",
    "#         for i in range(nn_model.config.num_hidden_layers):\n",
    "#             base_attn_in = nn_model.model.layers[i].self_attn.o_proj.input[0][0].clone().save()\n",
    "#             nn_model.model.layers[i].self_attn.o_proj.input[0][0] = base_attn_in[0][0] + source_attn_in[0][0]\n",
    "\n",
    "#             post_interv = nn_model.model.layers[i].self_attn.o_proj.input[0][0].clone().save()\n",
    "\n",
    "#             # hidden_states_post = nn_model.model.layers[i].self_attn.o_proj.input.save()\n",
    "            \n",
    "\n",
    "# # print(attn_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "outs = intervene_llama_DAS_batch(inputs, 12, nn_model, learned_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 132, 32000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Q: control\\nA: controlled\\n\\nQ: discover\\nA: discovered\\n\\nQ: investigate\\nA: investigated\\n\\nQ: join\\nA: joined\\n\\nQ: form\\nA: formed\\n\\nQ: compete\\nA: competed\\n\\nQ: warn\\nA: warned\\n\\nQ: play\\nA: played\\n\\nQ: change\\nA: changed\\n\\nQ: accelerate\\nA: accelerated\\n\\nQ: let\\nA:</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['predictive_token_idxs'][:3]\n",
    "\n",
    "nn_model.tokenizer.decode(inputs['source_input_ids'][6].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 132, 32000])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn_model.tokenizer.decode(outs[torch.arange(3),inputs['predictive_token_idxs'][:3]].softmax(-1).argmax(-1))\n",
    "outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counterfactual_outputs.shape\n",
    "# del counterfactual_outputs\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intervene_llama_DAS(batch, layer, model, ii_proj):\n",
    "\n",
    "    batch_size = len(batch['base_input_ids'])\n",
    "    # batch_size = 2\n",
    "\n",
    "    with model.trace() as tracer:\n",
    "\n",
    "        for i  in range(batch_size):\n",
    "            base_prompt, source_prompt = batch['base_input_ids'][i], batch['source_input_ids'][i]\n",
    "            base_intervention_token_idx, source_intervention_token_idx = batch['predictive_token_idxs'][i].item(), batch['source_predictive_token_idxs'][i].item()\n",
    "\n",
    "            with tracer.invoke(base_prompt):\n",
    "                base = model.model.layers[layer].output[0]\n",
    "                # print(base)\n",
    "                \n",
    "            \n",
    "            with tracer.invoke(source_prompt):\n",
    "                source = model.model.layers[layer].output[0]\n",
    "        \n",
    "            B = base[:,base_intervention_token_idx,:].detach()\n",
    "            S = source[:,source_intervention_token_idx,:].detach()\n",
    "\n",
    "            # intervention\n",
    "            with tracer.invoke(base_prompt):\n",
    "                # pass\n",
    "                \n",
    "                X = ii_proj(B,S)\n",
    "                # print((B + S).sum())\n",
    "                # model.model.layers[layer].output[0][:,base_intervention_token_idx,:] = \n",
    "                # model.model.layers[layer].output[0].token[base_intervention_token_idx] = ii_proj(base[:,base_intervention_token_idx,:], source[:,source_intervention_token_idx,:])\n",
    "            \n",
    "        save_out = model.output.save()\n",
    "    \n",
    "    # print(base, base.value)\n",
    "    \n",
    "    return save_out.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def intervene_llama_DAS_batch(batch, layer, model, ii_proj):\n",
    "\n",
    "#     batch_size = len(batch['base_input_ids'])\n",
    "#     all_inds = torch.arange(batch_size)\n",
    "#     # batch_size = 2\n",
    "\n",
    "#     with model.trace() as tracer:\n",
    "\n",
    "#         # for i  in range(batch_size):\n",
    "#         base_prompt, source_prompt = batch['base_input_ids'], batch['source_input_ids']\n",
    "#         base_intervention_token_idx, source_intervention_token_idx = batch['predictive_token_idxs'], batch['source_predictive_token_idxs']\n",
    "\n",
    "#         with tracer.invoke(base_prompt):\n",
    "#             base = model.model.layers[layer].output[0]\n",
    "        \n",
    "#         with tracer.invoke(source_prompt):\n",
    "#             source = model.model.layers[layer].output[0]\n",
    "\n",
    "#         B = base[all_inds,base_intervention_token_idx,:].detach()\n",
    "#         S = source[all_inds,source_intervention_token_idx,:].detach()\n",
    "#         print(B.shape, S.shape)\n",
    "    \n",
    "#         # intervention\n",
    "#         with tracer.invoke(base_prompt):\n",
    "#             # base = model.model.layers[layer].output[0]\n",
    "#             # print(base.shape, base_intervention_token_idx, source.shape, source_intervention_token_idx)\n",
    "            \n",
    "#             model.model.layers[layer].output[0][all_inds,base_intervention_token_idx,:] = ii_proj(B, S)\n",
    "#                 # model.model.layers[layer].output[0].token[base_intervention_token_idx] = ii_proj(base[:,base_intervention_token_idx,:], source[:,source_intervention_token_idx,:])\n",
    "            \n",
    "#         save_out = model.output.save()\n",
    "    \n",
    "#     return save_out.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def intervene_llama_DAS(batch, layer, model, ii_proj):\n",
    "\n",
    "#     batch_size = len(batch['base_input_ids'])\n",
    "#     # batch_size = 2\n",
    "\n",
    "#     with model.trace(validate=False) as tracer:\n",
    "\n",
    "#         for i  in range(batch_size):\n",
    "#             base_prompt, source_prompt = batch['base_input_ids'][i], batch['source_input_ids'][i]\n",
    "#             base_intervention_token_idx, source_intervention_token_idx = batch['predictive_token_idxs'][i].item(), batch['source_predictive_token_idxs'][i].item()\n",
    "\n",
    "#             with tracer.invoke(base_prompt, scan=False):\n",
    "#                 base = model.model.layers[layer].output\n",
    "#                 print(base.shape)\n",
    "                \n",
    "            \n",
    "#             with tracer.invoke(source_prompt, scan=False):\n",
    "#                 source = model.model.layers[layer].output\n",
    "        \n",
    "#             # intervention\n",
    "#             with tracer.invoke(base_prompt, scan=False):\n",
    "#                 model.model.layers[layer].output[:,base_intervention_token_idx,:] = ii_proj(base[:,base_intervention_token_idx,:], source[:,source_intervention_token_idx,:])\n",
    "#                 # model.model.layers[layer].output[0].token[base_intervention_token_idx] = ii_proj(base[:,base_intervention_token_idx,:], source[:,source_intervention_token_idx,:])\n",
    "            \n",
    "#         save_out = model.output.save()\n",
    "    \n",
    "#     # print(base.value[0].shape)\n",
    "    \n",
    "#     return save_out.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m attn_in \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m nn_model\u001b[38;5;241m.\u001b[39mtrace() \u001b[38;5;28;01mas\u001b[39;00m tracer:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracer\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhello world base\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nn_model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers):\n",
      "File \u001b[0;32m~/.conda/envs/nns_test/lib/python3.10/site-packages/nnsight/contexts/Runner.py:41\u001b[0m, in \u001b[0;36mRunner.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"On exit, run and generate using the model whether locally or on the server.\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc_val, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_val\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremote:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_server()\n",
      "Cell \u001b[0;32mIn[22], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nn_model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers):\n\u001b[1;32m      6\u001b[0m         source_attn_in \u001b[38;5;241m=\u001b[39m nn_model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[i]\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mo_proj\u001b[38;5;241m.\u001b[39minput\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tracer\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhello world source\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nn_model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers):\n\u001b[1;32m     10\u001b[0m         base_attn_in \u001b[38;5;241m=\u001b[39m nn_model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[i]\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mo_proj\u001b[38;5;241m.\u001b[39minput[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/.conda/envs/nns_test/lib/python3.10/site-packages/nnsight/contexts/Invoker.py:86\u001b[0m, in \u001b[0;36mInvoker.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc_val, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m---> 86\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc_val\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracer\u001b[38;5;241m.\u001b[39m_invoker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nn_model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers):\n\u001b[1;32m     10\u001b[0m     base_attn_in \u001b[38;5;241m=\u001b[39m nn_model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[i]\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mo_proj\u001b[38;5;241m.\u001b[39minput[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mo_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m base_attn_in[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m source_attn_in[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     13\u001b[0m     post_interv \u001b[38;5;241m=\u001b[39m nn_model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[i]\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mo_proj\u001b[38;5;241m.\u001b[39minput[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# hidden_states_post = nn_model.model.layers[i].self_attn.o_proj.input.save()\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/nns_test/lib/python3.10/site-packages/nnsight/tracing/Proxy.py:93\u001b[0m, in \u001b[0;36mProxy.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: Union[Proxy, Any], value: Union[Self, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetitem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nns_test/lib/python3.10/site-packages/nnsight/tracing/Node.py:198\u001b[0m, in \u001b[0;36mNode.add\u001b[0;34m(self, target, value, args, kwargs, name)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Proxy(\n\u001b[1;32m    188\u001b[0m         Node(\n\u001b[1;32m    189\u001b[0m             name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    195\u001b[0m         )\n\u001b[1;32m    196\u001b[0m     )\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m--> 198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nns_test/lib/python3.10/site-packages/nnsight/tracing/Graph.py:146\u001b[0m, in \u001b[0;36mGraph.add\u001b[0;34m(self, target, value, args, kwargs, name)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m FakeTensorMode(\n\u001b[1;32m    141\u001b[0m         allow_non_fake_inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    142\u001b[0m         shape_env\u001b[38;5;241m=\u001b[39mShapeEnv(assume_static_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m    143\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m fake_mode:\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m FakeCopyMode(fake_mode):\n\u001b[0;32m--> 146\u001b[0m             value \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mNode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_proxy_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mNode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_proxy_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m target_name \u001b[38;5;241m=\u001b[39m target \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m target\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname_idx:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "attn_in = []\n",
    "attn_out = []\n",
    "with nn_model.trace() as tracer:\n",
    "    with tracer.invoke('hello world base'):\n",
    "        for i in range(nn_model.config.num_hidden_layers):\n",
    "            source_attn_in = nn_model.model.layers[i].self_attn.o_proj.input.save()\n",
    "            \n",
    "    with tracer.invoke('hello world source'):\n",
    "        for i in range(nn_model.config.num_hidden_layers):\n",
    "            base_attn_in = nn_model.model.layers[i].self_attn.o_proj.input[0][0].clone().save()\n",
    "            nn_model.model.layers[i].self_attn.o_proj.input[0][0] = base_attn_in[0][0] + source_attn_in[0][0]\n",
    "\n",
    "            post_interv = nn_model.model.layers[i].self_attn.o_proj.input[0][0].clone().save()\n",
    "\n",
    "            # hidden_states_post = nn_model.model.layers[i].self_attn.o_proj.input.save()\n",
    "            \n",
    "\n",
    "# print(base_attn_in, post_interv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model.trace():\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageModelProxy (argument_32): ((FakeTensor(..., device='cuda:0', size=(1, 4, 4096)),), {})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_attn_in"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
